# -*- coding: utf-8 -*-
"""FML_22M0747_22M0750_22M0761_22M0770_22M0780

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18n_X3YoaYcKPqyiBE4NyoctUHl3IxopG

# Importing Libraries and Dataset
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

#Logistic Regression
from sklearn.linear_model import LogisticRegression


from sklearn.metrics import accuracy_score
from sklearn.metrics import ConfusionMatrixDisplay

file_url = 'https://raw.githubusercontent.com/bs-cse-iitb/FML_Project/main/diabetes_full.csv'
dataset = pd.read_csv(file_url)

"""# Exploratory Data Analysis 

The dataset has 20 Features columns and 1 Target column with 253680 records in total.

"Diabetes_012" column Represents the target variable 
"""

print(dataset.info())

"""### Checking for Missing Values

We can see that their are no null values in the dataeset
"""

dataset.isnull().sum()

"""## Checking the frequency of target variable classes 

Here class 0 represent No Diabetes, class 1 reperesent Pre Diabetes 
and class 2 represent Diabetes 

We can see that the data is skewed and hence we are combinining class 1 and 2 into a single class 1 (representing Diabetes)
"""

class_index = list(dataset.Diabetes_012.value_counts().index)
class_frequency = list(dataset.Diabetes_012.value_counts().values)

sns.set_style('darkgrid')
plt.pie(dataset.Diabetes_012.value_counts(),labels=["No Diabetes","Prediabetes","Diabetes"],autopct='%.2f')
plt.savefig('target_012_pie.png',bbox_inches="tight")

sns.barplot(class_index,class_frequency)
plt.savefig('target_012_barplot.png',bbox_inches='tight')

#Combining class 1 and 2 into a single class 1 

dataset['Diabetes_012'] = np.where(dataset['Diabetes_012'] == 0.0,0.0,1.0)
dataset.Diabetes_012.value_counts()
dataset.rename(columns={'Diabetes_012':'Diabetes_binary'},inplace=True)
print(dataset.Diabetes_binary.value_counts())

"""## Plotting the Correlation of every pair of columns"""

plt.figure(figsize = (20,15))
sns.heatmap(dataset.corr(),annot=True,linewidths=0.5,cmap='coolwarm')

"""## Plotting the Correlation of features against target column

 Fruits, Veggies, AnyHealthcare, NoDocbcCost,Sex have the lowest correlation with the target variable
"""

corr = dataset.corr()
graph = sns.barplot(corr.Diabetes_binary.index,corr.Diabetes_binary.values)
graph.set_xticklabels(labels=corr.Diabetes_binary.index, rotation=90)

plt.show()

plt.savefig("target_corr_bar.png",bbox_inches='tight')

"""Dropping the columns with lowest correlation"""

variables_to_drop = ['Fruits','Veggies','AnyHealthcare','NoDocbcCost','Sex']
dataset.drop(variables_to_drop,axis=1,inplace=True)

"""# Logistic Regression

## Preprocessing
"""

data = dataset.drop_duplicates()

y = data.Diabetes_binary
X = data.drop('Diabetes_binary',axis = 1)

#One Hot Encoding 
continous_var_list = []
categorical_var  = []
for var in X.columns:
  if var not in continous_var_list:
    categorical_var.append(var)

X = pd.get_dummies(X,columns=categorical_var)

from imblearn.under_sampling import NearMiss,CondensedNearestNeighbour,TomekLinks,EditedNearestNeighbours

# nm = TomekLinks() # 0.835876 - 15min
# nm = EditedNearestNeighbours(n_neighbors=3) # 0.851282 - 12min (n=3)
# nm = NearMiss(version = 1 , n_neighbors = 20) #0.856585 - 5min (n=20) // 0.853439 when 1-Hot done after undersampling
# nm = NearMiss(version = 1 , n_neighbors = 10) #0.856900 - 4min (n=10) // 0.849852 when 1-Hot done after undersampling
# nm = NearMiss(version = 1 , n_neighbors = 5) #0.844755 - 3min (n=5)

from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
nm = SMOTE() # 0.872103 - 48sec
# nm = ADASYN() # 0.869746

x_sm,y_sm= nm.fit_resample(X,y)
X,y=x_sm,y_sm 

print(X.shape)

"""## Train Test Split"""

X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,train_size=0.8)

"""##Trying Different Scaling Methods"""

from sklearn.preprocessing import MaxAbsScaler,StandardScaler,MinMaxScaler,RobustScaler,PowerTransformer,QuantileTransformer
scaler = MinMaxScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""## Grid Seach Implementation for logistic Regression"""

best_model = None
inv_regularization_constant = None
l1_ratio = None 
max_accuracy = 0.0
for i in np.arange(0.1,1.1,0.2):
  for j in np.arange(0.1,1.1,0.2):

    logisticRegressionModel = LogisticRegression(max_iter = 1000, solver='saga', penalty='elasticnet', C=i, l1_ratio=j, tol=1e-3, warm_start=True)
    logisticRegressionModel.fit(X_train, y_train)
    y_pred = logisticRegressionModel.predict(X_test)
    current_accuracy = accuracy_score(y_test, y_pred)
    if(current_accuracy > max_accuracy):
      best_model = logisticRegressionModel
      max_accuracy = current_accuracy
      l1_ratio = j
      inv_regularization_constant  = i

print("Logistic Regression Test Maximum Accuracy = "+ str(max_accuracy) + " , obtained for Inverse Regularization Constant = "+str(inv_regularization_constant) + " and L1 Regularization Ratio = "+str(l1_ratio))

logisticRegressionModel = best_model

"""## Evaluating Model Performance"""

Y_pred = logisticRegressionModel.predict(X_test)
Y_val = logisticRegressionModel.predict(X_train)

from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay

print("~~~~~~~  TRAIN ACCURACY  ~~~~~~~")
print(classification_report(y_train, Y_val, digits=6))

print("~~~~~~~  TEST ACCURACY  ~~~~~~~")
print(classification_report(y_test, Y_pred, digits=6))

"""# Naive Bayes

## Preprocessing

Since Naive Bayes has an assumption that features are conditionally independent hence removing the columns which have high correlation with each other
"""

df = dataset

#Identifying the columns with high correlation among each other

cor_matrix=df.corr().abs()
upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))
[column for column in upper_tri.columns if any(upper_tri[column] > 0.32)]

#Columns with high correlation
df.drop(['PhysHlth', 'Income', 'DiffWalk', 'MentHlth'],axis=1,inplace=True)

X = df.drop(['Diabetes_binary'], axis=1)
y = df['Diabetes_binary']

X = np.asarray(X)
y = np.asarray(y)

"""## Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

"""## Scaling the data using Standard Scaler"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""## Training Naive Bayes Classifier"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)

"""## Evaluating Model Performance"""

y_val = gnb.predict(X_train)
y_pred = gnb.predict(X_test)

print("~~~~~~~  TRAIN ACCURACY  ~~~~~~~")
print(classification_report(y_train, y_val, digits=6))

print("~~~~~~~  TEST ACCURACY  ~~~~~~~")
print(classification_report(y_test, y_pred, digits=6))

"""#Neural Network

## Preprocessing
"""

from imblearn.under_sampling import NearMiss,CondensedNearestNeighbour,TomekLinks,EditedNearestNeighbours
from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN

y = data.Diabetes_binary
X = data.drop('Diabetes_binary',axis = 1)

X = np.asarray(X)
y = np.asarray(y)

# nm = TomekLinks() # accuracy: 0.8358 - 10min
# nm = EditedNearestNeighbours(n_neighbors=3) # accuracy: 0.8560 - 13min (n=3)
nm = NearMiss(version = 1 , n_neighbors = 20) #accuracy: 0.8626 - 5min (n=20) 
# nm = NearMiss(version = 1 , n_neighbors = 10) #accuracy: 0.8559 - 4min (n=10) 
# # nm = NearMiss(version = 1 , n_neighbors = 5) #0.844755 - 3min (n=5)

# from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
# nm = SMOTE() # accuracy: 0.8314
# nm = ADASYN() # accuracy: 0.8213

x_sm,y_sm= nm.fit_resample(X,y)
X,y=x_sm,y_sm

"""## Train Test Split"""

X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,train_size=0.8)

"""## Scaling the data using Robust Scaler"""

from sklearn.preprocessing import MaxAbsScaler,StandardScaler,MinMaxScaler,RobustScaler,PowerTransformer,QuantileTransformer

scaler = RobustScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)  

# minmax - accuracy: 0.8537
# maxabs - accuracy: 0.8499
# standard - accuracy: 0.8634
# robust - accuracy: 0.8658
# power - accuracy: 0.8643
# quantile - accuracy: 0.8520

"""## Training Neural NetWork Classifier"""

from sklearn.metrics import accuracy_score
from keras.models import Sequential
from keras.layers import Dense
import keras.layers as layers
import keras

# Grid Search NN
max_accuracy = 0.0
best_model = None
learning_rate = None
batch_size = None
epochs = None

for i in {0.001,0.0001}:
  for j in {64,32}:
    for k in {30}:
      
      model = Sequential()
      model.add(layers.InputLayer((X_train.shape[1],)))
      model.add(Dense(32,activation='relu'))
      model.add(Dense(64,activation='relu'))
      model.add(Dense(1,activation='sigmoid'))

      optimizer = keras.optimizers.Adam(learning_rate = i)
      model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
      history = model.fit(X_train,y_train,batch_size=j,epochs=k,validation_split=0.2)
      p_pred = model.predict(X_test)
      p_pred = p_pred.flatten()

      # extract the predicted class labels
      y_pred = np.where(p_pred > 0.5, 1, 0)
      
      current_accuracy = accuracy_score(y_test,y_pred)
      if(current_accuracy > max_accuracy):
        best_model = model
        max_accuracy = current_accuracy
        learning_rate = i
        batch_size = j
        epochs = k

print("Neural Network Test Maximum Accuracy = "+ str(max_accuracy) + " , obtained for Learning Rate = "+str(learning_rate) + " and batch size = "+str(batch_size) + " and epochs = " + str(epochs))


neuralNetworkModel = best_model

import matplotlib.pyplot as plt
fig = plt.figure(figsize=(10, 6))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
from google.colab import files
plt.savefig("nn1.pdf", bbox_inches = 'tight')
files.download("nn1.pdf")

fig = plt.figure(figsize=(10, 6))
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')

from google.colab import files
plt.savefig("nn2.pdf", bbox_inches = 'tight')
files.download("nn2.pdf")

"""## Evaluting Model Performance"""

y_val = neuralNetworkModel.predict(X_train)

# extract the predicted class labels
y_val = np.where(y_val > 0.5, 1, 0)
y_pred = neuralNetworkModel.predict(X_test)

y_pred = y_pred.flatten()

# extract the predicted class labels
y_pred = np.where(p_pred > 0.5, 1, 0)

print("~~~~~~~  TRAIN ACCURACY  ~~~~~~~")
print(classification_report(y_train, y_val, digits=6))

print("~~~~~~~  TEST ACCURACY  ~~~~~~~")
print(classification_report(y_test, y_pred, digits=6))

"""# Decision Trees

## Preprocessing
"""

y = data.Diabetes_binary
X = data.drop('Diabetes_binary',axis = 1)

#One Hot Encoding of categorical data
continous_var_list = ['BMI','MentHlth','PhysHlth']

categorical_var  = []
for var in X.columns:
  if var not in continous_var_list:
    categorical_var.append(var)
print(categorical_var)
X = pd.get_dummies(X,columns=categorical_var)

#Undersampling to remove the skew of that dataset
from imblearn.under_sampling import NearMiss
nm = NearMiss(version = 1 , n_neighbors = 10)

# from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
# # nm = SMOTE()  # 0.872103
# nm = ADASYN()

x_sm,y_sm= nm.fit_resample(X,y)
X = x_sm
y = y_sm

"""##Scaling the data using MinMax Scaler"""

X = np.asarray(X)
y = np.asarray(y)
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,train_size=0.7)

scaler = MinMaxScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""##Training Decision Trees Classifier"""

from sklearn import tree

def deci(x_train, y_train, x_test, y_test, sample_weight=None, dep=1):
  finalModel = None
  maxAcc = 0
  no_of_features = x_train.shape[1]
  for depth in range(1,int(no_of_features*dep),1):
    model = tree.DecisionTreeClassifier(max_depth=depth, criterion="gini")
    if sample_weight == None:
      model.fit(x_train, y_train)
    else:
      model.fit(x_train, y_train, sample_weight=sample_weight)
    scoreTrain = model.score(x_train,y_train)
    scoreTest = model.score(x_test, y_test)
    if(scoreTrain - scoreTest <= 0.02 and scoreTrain>maxAcc):
      finalModel = model
      maxAcc = scoreTrain
  return finalModel

decision_model = deci(X_train, y_train, X_test, y_test)

"""## Evaluating Model Performance"""

y_val = decision_model.predict(X_train)
y_pred = decision_model.predict(X_test)

print("~~~~~~~  TRAIN ACCURACY  ~~~~~~~")
print(classification_report(y_train, y_val, digits=6))

print("~~~~~~~  TEST ACCURACY  ~~~~~~~")
print(classification_report(y_test, y_pred, digits=6))

"""# Random Forest

##Preprocessing
"""

y = data.Diabetes_binary
X = data.drop('Diabetes_binary',axis = 1)

continous_var_list = ['BMI','MentHlth','PhysHlth']

categorical_var  = []
for var in X.columns:
  if var not in continous_var_list:
    categorical_var.append(var)
print(categorical_var)
X = pd.get_dummies(X,columns=categorical_var)

from imblearn.under_sampling import NearMiss
nm = NearMiss(version = 1 , n_neighbors = 10)

# from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
# # nm = SMOTE()  # 0.872103
# nm = ADASYN()

x_sm,y_sm= nm.fit_resample(X,y)
X = x_sm
y = y_sm

"""## Scaling the data with MinMax Scaler"""

X = np.asarray(X)
y = np.asarray(y)
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,train_size=0.7)

# scaler = MaxAbsScaler().fit(X_train)
scaler = MinMaxScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""## Training Random Forest Classifier"""

def deci(x_train, y_train, x_test, y_test, sample_weight=None, dep=1):
  finalModel = None
  maxAcc = 0
  no_of_features = x_train.shape[1]
  for depth in range(1,int(no_of_features*dep),1):
    model = tree.DecisionTreeClassifier(max_depth=depth, criterion="gini")
    if sample_weight == None:
      model.fit(x_train, y_train)
    else:
      model.fit(x_train, y_train, sample_weight=sample_weight)
    scoreTrain = model.score(x_train,y_train)
    scoreTest = model.score(x_test, y_test)
    if(scoreTrain - scoreTest <= 0.02 and scoreTrain>maxAcc):
      finalModel = model
      maxAcc = scoreTrain
  return finalModel

def givePredict(x, aggModels, featureDel):
  predictedY = 0

  for j in range(len(aggModels)):
    model=aggModels[j]
    new_x = np.delete(x, featureDel[j], 1)
    p = int(model.predict(new_x)[0])
    if p==0:
        p=-1
    predictedY+=p
  if(predictedY>=0):
    return 1
  else:
    return 0

def randomForestPredict(x,aggModels,featureDel):
  predictions = list()
  for i in range(len(x)):
    inpA = np.array([x[i]])
    predictedY = 0
    
    predictedY = givePredict(inpA, aggModels, featureDel)
    
    predictions.append(predictedY)

  return predictions

def randomForest(X_train, y_train, X_test, y_test, no_of_trees=50):
  import random
  x,y = X_train, y_train
  aggModels = list()
  featureDel = list()
  for i in range(no_of_trees):
    n = len(x)
    newX, newY = list(), list()
    
    for j in range(n):
      index = random.randint(0,n-1)
      newX.append(x[index])
      newY.append(y[index])
    newX, newY = np.array(newX), np.array(newY)

    cols = newX.shape[1]
    d = random.randint(1,10)*0.1
    todel = random.sample(range(0, cols-1), int(cols*(1-d)))
    newX = np.delete(newX, todel, 1)

    newXtrain, newXtest, newYtrain, newYtest = train_test_split(newX,newY,random_state=42,train_size=0.8)
    sample_weight = [1/len(newXtrain) for i in range(len(newXtrain))]
    m = deci(newXtrain, newYtrain, newXtest, newYtest, sample_weight, 0.2)
    if m is not None:
      aggModels.append(m)
      featureDel.append(todel)

  return aggModels, featureDel

aggModels, featureDel = randomForest(X_train, y_train, X_test, y_test, 100)

"""## Evaluating Model Performance"""

from sklearn.metrics import classification_report
y_val = randomForestPredict(X_train,aggModels,featureDel)
y_pred = randomForestPredict(X_test,aggModels,featureDel)

print("~~~  TRAIN ACCURACY  ~~~")
print(classification_report(y_train, y_val, digits=6))

print("~~~  TEST ACCURACY  ~~~")
print(classification_report(y_test, y_pred, digits=6))

"""#Support Vector Machines

## Preprocessing
"""

Y = data['Diabetes_binary']
X = data.drop(columns=['Diabetes_binary'])

# step one hot coding

#for FIRST data set
#continous_var_list = ['age', 'BMI' 'trestbps','chol','thalach'	'oldpeak']
continous_var_list = ['BMI','MentHlth','PhysHlth', 'Age']
categorical_var  = []
for var in X.columns:
  if var not in continous_var_list:
    categorical_var.append(var)


print(categorical_var)
X = pd.get_dummies(X,columns=categorical_var)

abs_scaler = MaxAbsScaler()
abs_scaler.fit(X)
scaled_data = abs_scaler.transform(X)
df_scaled = pd.DataFrame(scaled_data, columns = X.columns)

from imblearn.under_sampling import NearMiss
nm = NearMiss(version = 1, n_neighbors=10)
X_sm, Y_sm = nm.fit_resample(X,Y)
X,Y = X_sm, Y_sm

"""## Train Test Split"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, train_size=0.80)
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

"""## Training SVM Classifier"""

def SVM(X_train, Y_train, X_test, Y_test, kernel = 'rbf', type = 2):

  if type==1:
    parameters = {}
    #Linear
    if kernel=='linear':
      #parameters = {'C':[0.3, 0.6, 0.9, 3, 6, 9, 48, 99]}
      parameters = {'C':[0.1, 0.5, 1, 5, 10, 50, 100]}
      svc = SVC(kernel = 'linear')
    #Polynomial
    elif kernel=='poly':
      parameters = {'C':[0.1, 1, 3], 'degree':[2, 3, 4], 'gamma': [0.1, 1]}
      svc = SVC(kernel = 'poly')
    #RBF
    elif kernel=='rbf':
      parameters = {'C':[0.1, 0.5, 1, 5, 10, 50, 100], 'gamma': [0.1, 0.5, 1, 3, 6, 9]}
      # other c value [0.1,0.3,1, 0.6, 0.9, 3, 6, 9, 48, 99]
      svc = SVC(kernel = 'rbf')
    
    elif kernel =='NuSVC':
      svc = NuSVC()
    #best tol 0.1
    elif kernel=='rbf_best':
      parameters = {'C':[20], 'gamma': [1]}
      svc = SVC(kernel = 'rbf')
    else:
      print("Invalid Kernel Value")

    clf = GridSearchCV(svc, parameters, cv=5)
    clf.fit(X_train, Y_train)
    Y_train_pred = clf.predict(X_train)
    Y_test_pred = clf.predict(X_test)
    test_score = accuracy_score(Y_test_pred, Y_test)
    train_score = accuracy_score(Y_train_pred, Y_train)
    print("Train Score: ", train_score)
    print("Test score: ", test_score)

  elif (type ==2):
    clf = SVC(kernel='rbf', C=20, gamma=1)
    clf.fit(X_train, Y_train)
    Y_pred = clf.predict(X_test)
    accuracy_test = accuracy_score(Y_test, Y_pred)
    accuracy_train = accuracy_score(Y_train, clf.predict(X_train))
    return clf

  else:
    kernels = ['linear', 'rbf', 'poly']
    gammas = [0.1, 1, 10, 100]
    C = [0.1, 1, 10, 50, 100, 1000]
    degrees = [1]
    for kernel in kernels:  
      for c in C:
        if kernel == 'linear':
          gammas = [1]
        for gamma in gammas:
          if kernel == 'poly':
            degrees = [0, 1, 2, 3, 4, 5, 6]
          for degree in degrees:      
            clf = SVC(kernel=kernel, C=c,degree=degree)
            clf.fit(X_train, Y_train)
            Y_pred = clf.predict(X_test)
            accuracy_test = accuracy_score(Y_test, Y_pred)
            accuracy_train = accuracy_score(Y_train, clf.predict(X_train))
            print("Kernel : {}, C : {}, gammas : {}, degrees :{} accuracy_train= {}, accuracy_test {}".format(kernel,c,gamma,degree,accuracy_train,accuracy_test))
          # degree is required only for polynomial
          degrees = [1]
          gammas = [0.1, 1, 10, 100]

svm_model = SVM(X_train, Y_train, X_test, Y_test)

"""## Evaluating Model Performance"""

y_val = svm_model.predict(X_train)
y_pred = svm_model.predict(X_test)


print("~~~  TRAIN ACCURACY  ~~~")
print(classification_report(Y_train, y_val, digits=6))

print("~~~  TEST ACCURACY  ~~~")
print(classification_report(Y_test, y_pred, digits=6))

y_val = svm_model.predict(X_train)
y_pred = svm_model.predict(X_test)

print("~~~~~~~  TRAIN ACCURACY  ~~~~~~~")
print(classification_report(y_train, y_val, digits=6))

print("~~~~~~~  TEST ACCURACY  ~~~~~~~")
print(classification_report(y_test, y_pred, digits=6))

"""# Boosting with Models of Same Class

## Preprocessing
"""

data = pd.read_csv("https://raw.githubusercontent.com/bs-cse-iitb/FML_Project/main/diabetes_full.csv")
data = data.drop_duplicates()

data['Diabetes_012'] = np.where(data['Diabetes_012'] == 0.0,0.0,1.0)
data.rename(columns={'Diabetes_012':'Diabetes_binary'},inplace=True)

y = data.Diabetes_binary
X = data.drop('Diabetes_binary',axis = 1)

continous_var_list = ['BMI','MentHlth','PhysHlth']

categorical_var  = []
for var in X.columns:
  if var not in continous_var_list:
    categorical_var.append(var)
print(categorical_var)
X = pd.get_dummies(X,columns=categorical_var)

from imblearn.under_sampling import NearMiss
nm = NearMiss(version = 1 , n_neighbors = 10)


# from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
# # nm = SMOTE()  # 0.872103
# nm = ADASYN()

x_sm,y_sm= nm.fit_resample(X,y)
X = x_sm
y = y_sm

X = np.asarray(X)
y = np.asarray(y)

"""## Train Test Split"""

X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,train_size=0.7)

from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler
scaler = MinMaxScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""## Decision Stump Definition"""

def decisionStumpClassifier(x_train, y_train,sample_weights,x_test):
  finalModel = None
  maxAcc = 0
  no_of_features = x_train.shape[1]

  for criterion in ["entropy", "gini"]:
    for max_features in ["sqrt", "log2"]:
      model = tree.DecisionTreeClassifier(max_depth=1, criterion=criterion, max_features=max_features)
      model.fit(x_train, y_train, sample_weight=sample_weights)
      scoreTrain = model.score(x_train,y_train)
      scoreTest = model.score(x_test, y_test)
      if(scoreTrain - scoreTest <= 0.02 and scoreTrain>maxAcc):
        finalModel = model
        maxAcc = scoreTrain

  pred = finalModel.predict(x_train)
  train_score = accuracy_score(pred, y_train)

  print("Decision Tree Training Accuracy: ", train_score)
  return finalModel,pred

def decisionStumpPredictions(X_test,model):
  pred = model.predict(X_test)
  return pred

"""## Training the Model"""

def decisionStumpClassifier(x_train, y_train,sample_weights,x_test):
  finalModel = None
  maxAcc = 0
  no_of_features = x_train.shape[1]

  for criterion in ["entropy", "gini"]:
    for max_features in ["sqrt", "log2"]:
      model = tree.DecisionTreeClassifier(max_depth=1, criterion=criterion, max_features=max_features)
      model.fit(x_train, y_train, sample_weight=sample_weights)
      scoreTrain = model.score(x_train,y_train)
      scoreTest = model.score(x_test, y_test)
      if(scoreTrain - scoreTest <= 0.02 and scoreTrain>maxAcc):
        finalModel = model
        maxAcc = scoreTrain

  pred = finalModel.predict(x_train)
  train_score = accuracy_score(pred, y_train)

  print("Decision Tree Training Accuracy: ", train_score)
  return finalModel,pred

def decisionStumpPredictions(X_test,model):
  pred = model.predict(X_test)
  return pred

"""## Evaluating Model Performance"""

final_predictions_test = np.zeros(X_test.shape[0])
for i in range(len(trained_classifier_list)):
  pred = decisionStumpPredictions(X_test,trained_classifier_list[i])
  pred = list(map(lambda x : -1 if x==0 else 1,pred))
  final_predictions_test+= [alpha_list[i]*x for x in pred]

final_predictions_test = list(map(lambda x : 1 if x>=0 else 0 , final_predictions_test))

from sklearn.metrics import classification_report

print(classification_report(final_predictions_test, y_test, digits=6))

final_predictions_train = np.zeros(X_train.shape[0])
for i in range(len(trained_classifier_list)):
  pred = decisionStumpPredictions(X_train,trained_classifier_list[i])
  pred = list(map(lambda x : -1 if x==0 else 1,pred))
  final_predictions_train+= [alpha_list[i]*x for x in pred]

final_predictions_train = list(map(lambda x : 1 if x>=0 else 0 , final_predictions_train))

from sklearn.metrics import classification_report

print(classification_report(final_predictions_train, y_train, digits=6))

"""# Hybrid Boosting

## Preprocessing
"""

data = pd.read_csv("https://raw.githubusercontent.com/bs-cse-iitb/FML_Project/main/diabetes_full.csv")
data = data.drop_duplicates()

data['Diabetes_012'] = np.where(data['Diabetes_012'] == 0.0,0.0,1.0)
data.rename(columns={'Diabetes_012':'Diabetes_binary'},inplace=True)

y = data.Diabetes_binary
X = data.drop('Diabetes_binary',axis = 1)

continous_var_list = ['BMI','MentHlth','PhysHlth']

categorical_var  = []
for var in X.columns:
  if var not in continous_var_list:
    categorical_var.append(var)
print(categorical_var)
X = pd.get_dummies(X,columns=categorical_var)

from imblearn.under_sampling import NearMiss
nm = NearMiss(version = 1 , n_neighbors = 10)


# from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
# # nm = SMOTE()  # 0.872103
# nm = ADASYN()

x_sm,y_sm= nm.fit_resample(X,y)
X = x_sm
y = y_sm

no_of_features = len(X.columns)

X = np.asarray(X)
y = np.asarray(y)

"""## Train Test Split"""

X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,train_size=0.7)

from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler
scaler = MinMaxScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""## Defining Functions for Classifiers

### Naive Bayes
"""

def naiveBayesClassifier(X_train,y_train,sample_weights,X_test):
  from sklearn.naive_bayes import GaussianNB
  gnb = GaussianNB()
  gnb.fit(X_train, y_train,sample_weight=sample_weights)
  y_pred = gnb.predict(X_train)
  train_score = accuracy_score(y_pred, y_train)

  print("Naive Bayes Training Accuracy: ", train_score)
  return gnb,y_pred

def naiveBayesPredictions(X_test,model):
  pred = model.predict(X_test)
  return pred

"""### Logistic Classifier"""

def logisticRegressionClassifier(X_train,y_train,sample_weights,X_test):
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import accuracy_score

  best_model = None
  max_accuracy = 0.0
  for i in np.arange(0.1,1.1,0.2):
    for j in np.arange(0.1,1.1,0.2):

      logisticRegressionModel = LogisticRegression(max_iter = 300, solver='saga', penalty='elasticnet', C=i, l1_ratio=j, tol=1e-1, warm_start=True)
      logisticRegressionModel.fit(X_train, y_train, sample_weight = sample_weights)
      y_pred = logisticRegressionModel.predict(X_test)
      current_accuracy = accuracy_score(y_test, y_pred)
      if(current_accuracy > max_accuracy):
        best_model = logisticRegressionModel
        max_accuracy = current_accuracy

  y_pred = best_model.predict(X_train)
  train_score = accuracy_score(y_pred, y_train)

  print("Logistic Regression Training Accuracy: ", train_score)
  return best_model,y_pred

def logisticRegressionPredictions(X_test,model):

  pred = model.predict(X_test)
  return pred

"""### Decision Trees"""

def decisionTreeClassifier(x_train, y_train,sample_weights,x_test):
  finalModel = None
  maxAcc = 0
  no_of_features = x_train.shape[1]
  for depth in range(1,int(no_of_features),2):
    for criterion in ["entropy", "gini"]:
      for max_features in ["sqrt", "log2"]:
        model = tree.DecisionTreeClassifier(max_depth=depth, criterion=criterion, max_features=max_features)
        model.fit(x_train, y_train, sample_weight=sample_weights)
        scoreTrain = model.score(x_train,y_train)
        scoreTest = model.score(x_test, y_test)
        if(scoreTrain - scoreTest <= 0.02 and scoreTrain>maxAcc):
          finalModel = model
          maxAcc = scoreTrain

  pred = finalModel.predict(x_train)
  train_score = accuracy_score(pred, y_train)

  print("Decision Tree Training Accuracy: ", train_score)
  return finalModel,pred

def decisionTreePredictions(X_test,model):
  pred = model.predict(X_test)
  return pred

"""### Neural Network"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
import keras.layers as layers
import keras
def neuralNetworkClassifier(x_train,y_train,sample_weights,x_test):
  model = Sequential()
  # Grid Search NN
  max_accuracy = 0.0
  best_model = None

  for i in {0.001}:
    for j in {64}:
      for k in {30}:

        model.add(layers.InputLayer((x_train.shape[1],)))
        model.add(Dense(32,activation='relu'))
        model.add(Dense(64,activation='relu'))
        model.add(Dense(1,activation='sigmoid'))

        optimizer = keras.optimizers.Adam(learning_rate = i)
        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'],weighted_metrics=[tf.keras.losses.binary_crossentropy])
        history = model.fit(x_train,y_train,batch_size=j,epochs=k,validation_split=0.2,sample_weight=sample_weights,verbose=0)
        p_pred = model.predict(x_test)
        p_pred = p_pred.flatten()

        # extract the predicted class labels
        y_pred = np.where(p_pred > 0.5, 1, 0)
        
        current_accuracy = accuracy_score(y_test,y_pred)
        if(current_accuracy > max_accuracy):
          best_model = model
          max_accuracy = current_accuracy

  pred = best_model.predict(x_train)
  pred = pred.flatten()
  pred = np.where(pred > 0.5, 1, 0)
  train_score = accuracy_score(pred, y_train)

  print("Neural Network Accuracy : ",train_score)
  return best_model,pred

def neuralNetworkPredictions(X_test,model):
  pred = model.predict(X_test)
  pred = pred.flatten()
  pred = np.where(pred>0.5,1,0)
  return pred

"""### SVM with Kernel"""

from sklearn.svm import SVC
from sklearn.svm import NuSVC
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV

def svmClassifier(X_train,Y_train,sample_weights,x_test):

  clf = SVC(tol=1e-2, kernel='rbf', C=1)
  clf.fit(X_train, Y_train, sample_weight = sample_weights)

  Y_pred = clf.predict(X_test)
  Y_train_pred = clf.predict(X_train)

  train_score = accuracy_score(Y_train_pred, y_train)

  print("SVM Training Accuracy: ", train_score)

  return clf,Y_train_pred

def svmPredictions(X_test,model):
  pred = model.predict(X_test)
  return pred

from sklearn import tree
from sklearn import ensemble

"""## Training Boosting Model"""

import math 

error_list = []
N = X_train.shape[0]
sample_weights = np.ones(N)

trained_classifier_list = []
alpha_list = []

classifier_function_list = [svmClassifier,neuralNetworkClassifier,logisticRegressionClassifier,decisionTreeClassifier,naiveBayesClassifier]



#Training Boosting Classifier
for classifier in classifier_function_list:

  #Training the model with sample weights 
  model,y_pred = classifier(X_train,y_train,sample_weights,X_test)

  #Calculating e^t
  error = 0
  for i in range(len(sample_weights)):
    if(y_pred[i] != y_train[i]):
      error+=sample_weights[i]

  #Calculating alpha = 1/2*ln((1-e^t)/e^t)

  error=error/N

  error_list.append(error)
  alpha = 1/2*(math.log((1-error)/error))

  #Storing alpha and classifer 
  alpha_list.append(alpha)
  trained_classifier_list.append(model)

  #Calculating Normalizing constant for constrianst sum of all weights = 1 
  Z = sum(sample_weights)/N

  #Updating Sample weights 
  for i in range(len(sample_weights)):
    if(y_pred[i] == y_train[i]):
      sample_weights[i] = (sample_weights[i]*(math.sqrt(error/(1-error))))/Z
    else:
      sample_weights[i] = (sample_weights[i]*(math.sqrt((1-error)/error)))/Z

"""## Evaluating Boosting Model"""

# prediciton_function_list = [naiveBayesPredictions,logisticRegressionPredictions, decisionTreePredictions, randomForestPredictions]
prediciton_function_list = [svmPredictions,neuralNetworkPredictions,logisticRegressionPredictions, decisionTreePredictions,naiveBayesPredictions]

final_predictions_test = np.zeros(X_test.shape[0])
for i in range(len(trained_classifier_list)):
  pred = prediciton_function_list[i](X_test,trained_classifier_list[i])
  pred = list(map(lambda x : -1 if x==0 else 1,pred))
  final_predictions_test+= [alpha_list[i]*x for x in pred]

final_predictions_test = list(map(lambda x : 1 if x>=0 else 0 , final_predictions_test))

from sklearn.metrics import classification_report

print(classification_report(final_predictions_test, y_test, digits=6))

final_predictions_train = np.zeros(X_train.shape[0])
for i in range(len(trained_classifier_list)):
  pred = prediciton_function_list[i](X_train,trained_classifier_list[i])
  pred = list(map(lambda x : -1 if x==0 else 1,pred))
  final_predictions_train+= [alpha_list[i]*x for x in pred]

final_predictions_train = list(map(lambda x : 1 if x>=0 else 0 , final_predictions_train))

from sklearn.metrics import classification_report

print(classification_report(final_predictions_train, y_train, digits=6))

"""##Intersection of missclassified points"""

!pip install venn

from matplotlib_venn import venn3, venn3_circles
from matplotlib import pyplot as plt

M = []
for i,classifier_model in enumerate(trained_classifier_list):
  pred = prediciton_function_list[i](X_train,trained_classifier_list[i])
  misclassified_set = set()
  for j in range(0,len(pred),1):
    if(pred[j] != y_train[j]):
      misclassified_set.add(j)
  M.append(misclassified_set)
  if(i==3):
    break

Ab = M[0].difference(M[1])
Ac = M[0].difference(M[2])
Ba = M[1].difference(M[0])
Bc = M[1].difference(M[2])
Ca = M[2].difference(M[0])

AB = M[0].intersection(M[1])
ABc = AB.difference(M[2])

AC = M[0].intersection(M[2])
ACb = AC.difference(M[1])

BC = M[1].intersection(M[2])
BCa = BC.difference(M[0])

ABC = AB.intersection(M[2])

Abc = Ab.difference(M[2])
Bac = Ba.difference(M[2])
Cab = Ca.difference(M[1])

fig = plt.figure(figsize=(10, 6))
venn3(subsets = (len(Abc), len(Bac), len(ABc), len(Cab), len(ACb), len(BCa), len(ABC)), set_labels = ('SVM', 'Neural Network', 'Logistic'), alpha = 0.5)
venn3_circles(subsets=(len(Abc), len(Bac), len(ABc), len(Cab), len(ACb), len(BCa), len(ABC)), linestyle="dashed", linewidth=2)

from google.colab import files
plt.savefig("venn.pdf", bbox_inches = 'tight')
files.download("venn.pdf")

from venn import venn


misclassified = {
    
    "NN" : M[0],
    "Logistic": M[1],
    "Decision": M[2],
    "Naive": M[3]
}

venn(misclassified)